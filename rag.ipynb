{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 開発実習環境セットアップ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1. Pythonのパッケージのインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai==0.27.8 \n",
    "!pip install langchain==0.0.240\n",
    "!pip install PyMuPDF==1.22.5\n",
    "!pip install tiktoken==0.4.0\n",
    "!pip install faiss-cpu==1.7.4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Pythonのモジュールのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# OpenAI\n",
    "import openai\n",
    "\n",
    "# LangChain\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Azure OpenAI Service\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import AzureChatOpenAI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. 環境変数の設定他"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".envファイルから環境変数をロードします。各環境にあわせて.env.templateを基に.evnファイルを作成してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getenv(\"OPENAI_API_VERSION\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Azure OpenAI ServiceのAPIを実行する際、デバッグレベルのログを出力するように設定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.log = \"debug\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. PDFファイルからベクトルデータベースのインデックス作成"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-1. PDFファイルを読み込み、分割"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "埋め込みモデルのinputの上限を超えず、情報源として役に立つサイズに分割します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file_path = \"./pdf/IBM_2021_ESG_Report.pdf\"\n",
    "loader = PyMuPDFLoader(pdf_file_path)\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    encoding_name=\"cl100k_base\",\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=200,\n",
    "    disallowed_special=(),\n",
    ")\n",
    "pages = loader.load_and_split(text_splitter=text_splitter)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PDFファイルを読み込んだ結果を確認してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_of_page = type(pages[0])\n",
    "page_count = len(pages)\n",
    "\n",
    "print(\"pageの型:\", type_of_page)\n",
    "print(\"ページ数:\", page_count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1ページ分のデータを参照してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = pages[11]\n",
    "formatted_data = json.dumps(page.__dict__, indent=4, ensure_ascii=False)\n",
    "print(formatted_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ページ内容\n",
    "result = page.page_content\n",
    "\n",
    "# ページ内容を見やすくするため、50文字ずつに改行して表示\n",
    "line_length = 50\n",
    "for i in range(0, len(result), line_length):\n",
    "    print(result[i:i+line_length])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. 埋め込みモデル（embedding）のインスタンス作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "埋め込みモデルには、Azure OpenAI Serviceを使います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBE_DEPLOYMENT_NAME = os.getenv(\"EMBE_DEPLOYMENT_NAME\")\n",
    "embeddings = OpenAIEmbeddings(deployment=EMBE_DEPLOYMENT_NAME, chunk_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "埋め込みモデルを使って、テキストをベクトル化してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"これは埋め込みのテストです。\"\n",
    "embeddings.embed_query(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-3. 埋め込みモデルを使って、PDFファイルからインデックスを作成する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ベクトルデータベースには、FAISSを使います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(pages, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "検索してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"IBMは気候変動のリスクにどのように取り組んでいますか？\"\n",
    "\n",
    "docs = db.similarity_search(query)\n",
    "for doc in docs:\n",
    "    print(\"page\", doc.metadata[\"page\"], doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "検索結果はデフォルトで4ドキュメントを含みます。それぞれ、PDFファイルのメタデータを持っています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_to_dict(document):\n",
    "    return document.__dict__\n",
    "\n",
    "formatted_data = json.dumps(docs[0], default=document_to_dict, indent=4, ensure_ascii=False)\n",
    "print(formatted_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "インデックスはローカルに保存することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_path = \"./index/IBM_2021_ESG_Report\"\n",
    "db.save_local(index_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存したインデックスをロードすることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.load_local(index_path, embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Retrieval QAチェーンの作成"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. LLMのインスタンス作成を作成する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMにAzure OpenAI Serviceを使います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAT_DEPLOYMENT_NAME = os.getenv(\"CHAT_DEPLOYMENT_NAME\")\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=CHAT_DEPLOYMENT_NAME,\n",
    "    temperature=0.5,)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Retrieval QAチェーンを作成する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "retriever（インデックス内の文書や埋め込みを検索するためのメソッド）とLLMを使って、Retrieval QAチェーンを生成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. QA実施"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieval QAチェーンを使って、QAします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"IBMの温室効果ガス排出量削減目標を教えてください。\"\n",
    "response = qa_chain({\"query\": query})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答を見やすく表示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# レスポンスの回答部分\n",
    "result = response[\"result\"]\n",
    "\n",
    "# 回答を見やすくするため、50文字ずつに改行して表示\n",
    "line_length = 50\n",
    "for i in range(0, len(result), line_length):\n",
    "    print(result[i:i+line_length])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "レスポンスを確認してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_to_dict(document):\n",
    "    return document.__dict__\n",
    "\n",
    "formatted_data = json.dumps(response, default=document_to_dict, indent=4, ensure_ascii=False)\n",
    "print(formatted_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
